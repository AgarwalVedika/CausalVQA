{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "random.seed(1234)\n",
    "import numpy as np; np.random.seed(1234)\n",
    "from utils_picking import my_read_old, intersect, round_percent, get_indices_diff_list_suffix, vqa_score_list, ch_atleast_once, worst_case_acc\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ques_type_dir = 0\n",
    "TEST_SPLIT = 'val2014'\n",
    "DIST_NORM = 1\n",
    "MODEL = 'snmn'\n",
    "\n",
    "orig_90_10_vs_edit_90_10 = 1  ##### which set splits for orig you want to test\n",
    "snmn_lr_25_e_4 = 0  # make sure snmn_lr_1e3_cvpr_check lr =0 \n",
    "snmn_lr_1e3_cvpr_check = 0  \n",
    "ques_type = MINI_DIR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "## orig_all vs edit_10\n",
    "edit_root_dir_qa = config.iv_q  \n",
    "\n",
    "coco_json = 'coco_cat_ids.json'\n",
    "\n",
    "#### questions keys: 'image_id', 'question', 'question_id'\n",
    "standard_questions_val_json = '/BS/databases10/VQA_v2/Questions/v2_OpenEnded_mscoco_'  + TEST_SPLIT + '_questions.json'\n",
    "res_file_q = 'v2_OpenEnded_mscoco_' + TEST_SPLIT + '_questions.pickle'\n",
    "standard_questions_edit_val_json = os.path.join(edit_root_dir_qa, res_file_q)\n",
    "\n",
    "\n",
    "## ann keys: 'image_id', 'question_id', 'answers' , 'multiple_choice_answer'(the most frequent answer), 'question_type', 'answer_type'\n",
    "standard_annotations_val_json = '/BS/databases10/VQA_v2/Annotations/v2_mscoco_'  + TEST_SPLIT + '_annotations.json'\n",
    "res_file_a = 'v2_mscoco_' + TEST_SPLIT + '_annotations.json'\n",
    "standard_annotations_edit_val_json = os.path.join(edit_root_dir_qa, res_file_a)\n",
    "\n",
    "if orig_90_10_vs_edit_90_10:\n",
    "    orig_root_dir_qa = '/BS/vedika2/nobackup/thesis/mini_datasets_qa_CNN_finetune_testing/' + ques_type + '/orig_90_10/'\n",
    "    edit_root_dir_qa = '/BS/vedika2/nobackup/thesis/mini_datasets_qa_CNN_finetune_testing/' + ques_type + '/edit_10/'\n",
    "    coco_json = '/BS/vedika2/nobackup/snmn/coco_cat_ids.json'\n",
    "\n",
    "    #### questions keys: 'image_id', 'question', 'question_id'\n",
    "    res_file_q = 'v2_OpenEnded_mscoco_' + TEST_SPLIT + '_questions.json'\n",
    "    standard_questions_val_json = os.path.join(orig_root_dir_qa, res_file_q)\n",
    "    standard_questions_edit_val_json = os.path.join(edit_root_dir_qa, res_file_q)\n",
    "\n",
    "\n",
    "    ## ann keys: 'image_id', 'question_id', 'answers' , 'multiple_choice_answer'(the most frequent answer), 'question_type', 'answer_type'\n",
    "    res_file_a = 'v2_mscoco_' + TEST_SPLIT + '_annotations.json'\n",
    "    standard_annotations_val_json = os.path.join(orig_root_dir_qa, res_file_a)\n",
    "    standard_annotations_edit_val_json = os.path.join(edit_root_dir_qa, res_file_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make changes here\n",
    "\n",
    "if MODEL == 'snmn':\n",
    "    results_edit_val_old = '/BS/vedika2/nobackup/snmn/exp_vqa/eval_outputs_vqa_v2/vqa_v2_scratch_train/vqa_v2_edited_val2014_vqa_v2_scratch_train_15000_results.pickle'\n",
    "    results_val_old = '/BS/vedika2/nobackup/snmn/exp_vqa/eval_outputs_vqa_v2/vqa_v2_scratch_train/vqa_v2_val2014_vqa_v2_scratch_train_15000_results.pickle'\n",
    "    ## standard_vocab file_model_specific\n",
    "    standard_vocab_ans_file = '/BS/vedika2/nobackup/snmn/exp_vqa/data/answers_vqa.txt'\n",
    "    with open(standard_vocab_ans_file) as f:\n",
    "        ans_vocab_list = f.read().splitlines()\n",
    "    \n",
    "    if snmn_lr_25_e_4:\n",
    "        results_edit_val_old = '/BS/vedika2/nobackup/snmn/exp_vqa/eval_outputs_vqa_v2/vqa_v2_scratch_train_25lr/vqa_v2_edited_val2014_vqa_v2_scratch_train_25lr_30_results.pickle'\n",
    "        results_val_old = '/BS/vedika2/nobackup/snmn/exp_vqa/eval_outputs_vqa_v2/vqa_v2_scratch_train_25lr/vqa_v2_val2014_vqa_v2_scratch_train_25lr_30_results.pickle'\n",
    "\n",
    "    if snmn_lr_1e3_cvpr_check:        \n",
    "        results_val_old =  '/BS/vedika2/nobackup/snmn/exp_vqa/eval_outputs_vqa_v2/vqa_v2_scratch_train_cvpr_check/vqa_v2_val2014_vqa_v2_scratch_train_cvpr_check_15_results.pickle'\n",
    "        results_edit_val_old =   '/BS/vedika2/nobackup/snmn/exp_vqa/eval_outputs_vqa_v2/vqa_v2_scratch_train_cvpr_check/vqa_v2_edited_val2014_vqa_v2_scratch_train_cvpr_check_15_results.pickle'\n",
    "        \n",
    "elif MODEL == 'SAAA':\n",
    "    results_edit_val_old ='/BS/vedika3/nobackup/pytorch-vqa/logs/edit_val_with_attn_.pickle'\n",
    "    results_val_old ='/BS/vedika3/nobackup/pytorch-vqa/logs/val_with_attn_.pickle'\n",
    "    ## standard_vocab file_model_specific\n",
    "    vocab = '/BS/vedika3/nobackup/pytorch-vqa/vocab.json'\n",
    "    with open(vocab, 'r') as f:\n",
    "        ans_vocab = json.load(f)[\"answer\"]\n",
    "        ans_vocab_list = [k for k, v in ans_vocab.items()]\n",
    "        #{v: k for k, v in ans_vocab.items()}   ### is a dictionary here but will work: keys- index- 0,1,2...\n",
    "\n",
    "elif MODEL == 'CNN_LSTM':\n",
    "    results_edit_val_old = '/BS/vedika3/nobackup/pytorch-vqa/logs/edit_val_no_attn_.pickle'\n",
    "    results_val_old = '/BS/vedika3/nobackup/pytorch-vqa/logs/val_no_attn_.pickle'\n",
    "    ## standard_vocab file_model_specific\n",
    "    vocab = '/BS/vedika3/nobackup/pytorch-vqa/vocab.json'\n",
    "    with open(vocab, 'r') as f:\n",
    "        ans_vocab = json.load(f)[\"answer\"]\n",
    "        ans_vocab_list = [k for k, v in ans_vocab.items()]\n",
    "        # {v: k for k, v in ans_vocab.items()}   ### is a dictionary here but will work: keys- index- 0,1,2...\n",
    "    dir_prefix = 'CNN_LSTM'\n",
    "    \n",
    "img_dir = '/BS/databases10/VQA_v2/Images/' + TEST_SPLIT + '/'\n",
    "edit_img_dir = '/BS/vedika2/work/thesis/final_edited_VQA_v2/Images/' + TEST_SPLIT + '/'\n",
    "img_prefix_name = 'COCO_' + TEST_SPLIT + '_'\n",
    "\n",
    "\n",
    "\n",
    "coco_json = '/BS/vedika2/nobackup/snmn/coco_cat_ids.json'\n",
    "with open(coco_json) as file:\n",
    "    coco_f = json.load(file)\n",
    "coco_dict = {}\n",
    "for idx, details in enumerate(coco_f):\n",
    "    # print(details)\n",
    "    coco_dict[details['id']] = details['name']\n",
    "coco_dict_inv = {}\n",
    "for idx, details in enumerate(coco_f):\n",
    "    # print(details)\n",
    "    coco_dict_inv[details['name']] = details['id']\n",
    "\n",
    "ques_type_file = '/BS/vedika3/nobackup/VQA_helper_tools_official/QuestionTypes/mscoco_question_types.txt'\n",
    "with open(ques_type_file) as f:\n",
    "    ques_type_off = f.read().splitlines()\n",
    "    \n",
    "ans_type_off = ['yes/no', 'number', 'other']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4199817180633545\n"
     ]
    }
   ],
   "source": [
    "qid_val, pred_ans_val , ss_vc_val, img_ids_val, ques_val, all_ans_val, ques_type_val, ans_type_val = my_read_old(results_val_old, standard_questions_val_json, standard_annotations_val_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.360442876815796\n"
     ]
    }
   ],
   "source": [
    "qid_edit_val, pred_ans_edit_val , ss_vc_edit_val, img_ids_edit_val, ques_edit_val, all_ans_edit_val, ques_type_edit_val, ans_type_edit_val = my_read_old(results_edit_val_old, standard_questions_edit_val_json, standard_annotations_edit_val_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108239\n"
     ]
    }
   ],
   "source": [
    "if orig_90_10_vs_edit_90_10:\n",
    "    masking_indices_where_qid_edit_but_no_orig_qid = [idx for idx, i in enumerate(qid_edit_val) if i not in qid_val]\n",
    "    stop_idx = masking_indices_where_qid_edit_but_no_orig_qid[0]\n",
    "    qid_edit_val = qid_edit_val[0:stop_idx]\n",
    "    qid_edit_val, pred_ans_edit_val, ss_vc_edit_val, img_ids_edit_val, ques_edit_val, all_ans_edit_val, ques_type_edit_val, ans_type_edit_val = [\n",
    "        i[0:stop_idx] for i in\n",
    "        [qid_edit_val, pred_ans_edit_val, ss_vc_edit_val, img_ids_edit_val, ques_edit_val, all_ans_edit_val,\n",
    "         ques_type_edit_val, ans_type_edit_val]]\n",
    "    print(stop_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of original set: 63219\n",
      "size of edited set: 108239\n",
      "no of unique images in val_set:  29376\n",
      "no of unique images in edit_val_set:  53855\n",
      "#unique questions in original set:  63219\n",
      "#unique questions in edited set:  45076\n",
      "not every question in orig_val made it to edit_val=> for  18143  questions- no legit edited IQA possible\n"
     ]
    }
   ],
   "source": [
    "len_val = len(img_ids_val)\n",
    "len_edit_val = len(img_ids_edit_val)\n",
    "all_indices_val = np.arange(len_val)\n",
    "all_indices_edit_val = np.arange(len_edit_val)\n",
    "\n",
    "print('size of original set:', len_val)\n",
    "print('size of edited set:', len_edit_val)\n",
    "print('no of unique images in val_set: ',len(set(img_ids_val)))\n",
    "print('no of unique images in edit_val_set: ',len(set(img_ids_edit_val)))\n",
    "print('#unique questions in original set: ',len(list(set(qid_val))) )\n",
    "print('#unique questions in edited set: ', len(list(set(qid_edit_val))))\n",
    "    \n",
    "if len(list(set(qid_val))) != len(list(set(qid_edit_val))) :\n",
    "    print('not every question in orig_val made it to edit_val=> for ', len(list(set(qid_val)))- len(list(set(qid_edit_val)))  ,' questions- no legit edited IQA possible')\n",
    "\n",
    "qid_gt_ans_label = {}\n",
    "for idx,a in enumerate(qid_val):\n",
    "    qid_gt_ans_label[a] = all_ans_val[idx]\n",
    "    \n",
    "qid_predans_val = {}\n",
    "for idx, a in enumerate(qid_val):\n",
    "    qid_predans_val.setdefault(a, []).append(pred_ans_val[idx])\n",
    "    \n",
    "qid_predans_idx_val = {}\n",
    "for idx, a in enumerate(qid_val):\n",
    "    qid_predans_idx_val.setdefault(a, []).append(idx)\n",
    "    \n",
    "#idx in case here refers to len(val and edit_val- order hai - so relax)    \n",
    "qid_predans_edit_val = {}\n",
    "for idx, a in enumerate(qid_edit_val):\n",
    "    qid_predans_edit_val.setdefault(a, []).append(pred_ans_edit_val[idx])\n",
    "    #qid_predans_edit_val[a] = (pred_ans_edit_val[idx])   \n",
    "    \n",
    "qid_predans_idx_edit_val = {}\n",
    "for idx, a in enumerate(qid_edit_val):\n",
    "    qid_predans_idx_edit_val.setdefault(a, []).append(idx)    \n",
    "    \n",
    "# qid_predans_imgid_edit_val = {}\n",
    "# for idx, a in enumerate(qid_edit_val):\n",
    "#     qid_predans_imgid_edit_val.setdefault(a, []).append(img_ids_edit_val[idx])   \n",
    "\n",
    "## creating dictionary for val set - to facilitate extensions based on q_id index\n",
    "qid_ss_predans_val = {}\n",
    "for idx, a in enumerate(qid_val):\n",
    "    qid_ss_predans_val[a] = (ss_vc_val[idx], pred_ans_val[idx], qid_val[idx],idx, all_ans_val[idx])\n",
    "\n",
    "extended_ss_vc_val = [qid_ss_predans_val[q_id][0] for q_id in qid_edit_val]\n",
    "extended_pred_ans_val = [qid_ss_predans_val[q_id][1] for q_id in qid_edit_val] \n",
    "#extended_qid_val = [qid_ss_predans_val[q_id][2] for q_id in qid_edit_val]\n",
    "\n",
    "collapsed_pred_ans_val = [qid_ss_predans_val[q_id][1] for q_id in set(qid_edit_val)] \n",
    "collapsed_all_ans_val = [qid_ss_predans_val[q_id][4] for q_id in set(qid_edit_val)] \n",
    "collapsed_indices = [qid_ss_predans_val[q_id][3] for q_id in set(qid_edit_val)] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy before editing, one answer match is good 66.039 41749\n",
      "accuracy before editing_10_ans_same, one answer match is good 66.039 63219\n",
      "accuracy before editing_extended 65.447 70839\n",
      "accuracy after editing, one answer match is good 65.146 70513\n",
      "accuracy before editing_collapsed 65.671\n",
      "worst case accuracy (45076):  63.178\n",
      "best case accuracy (45076):  67.517\n",
      "worst case official accuracy (45076):  63.178\n",
      "best case official accuracy (45076):  67.517\n",
      "official accuracy before editing 66.039\n",
      "official accuracy before editing_all_10_ans_same 66.039 63219\n",
      "official accuracy before editing_extended 65.447\n",
      "official accuracy after editing 65.146\n",
      "official accuracy before editing_collapsed 65.671\n"
     ]
    }
   ],
   "source": [
    "###                                         STATISTICS\n",
    "assert len(collapsed_indices) == len(set(qid_edit_val))\n",
    "\n",
    "ind_where_10_ans_same = [idx for idx, i in enumerate(all_ans_val) if len(set(i))==1 ]  \n",
    "\n",
    "## one match answer is good\n",
    "accuracy_ind_before = [i for i in range(len_val) if ans_vocab_list[pred_ans_val[i]] in all_ans_val[i]]\n",
    "\n",
    "accuracy_ind_bf_10_ans_same = [i for i in ind_where_10_ans_same if ans_vocab_list[pred_ans_val[i]] in all_ans_val[i]]\n",
    "\n",
    "accuracy_ind_before_extended = [i for i in range(len_edit_val) if ans_vocab_list[extended_pred_ans_val[i]] in all_ans_edit_val[i]]\n",
    "accuracy_ind_after = [i for i in range(len_edit_val) if ans_vocab_list[pred_ans_edit_val[i]] in all_ans_edit_val[i]]\n",
    "print('accuracy before editing, one answer match is good', \n",
    "      round_percent(len(accuracy_ind_before) / len_val), len(accuracy_ind_before))\n",
    "\n",
    "print('accuracy before editing_10_ans_same, one answer match is good', \n",
    "      round_percent(len(accuracy_ind_bf_10_ans_same) / len(ind_where_10_ans_same)), len(ind_where_10_ans_same))\n",
    "\n",
    "\n",
    "print('accuracy before editing_extended', \n",
    "      round_percent(len(accuracy_ind_before_extended) / len_edit_val), len(accuracy_ind_before_extended))\n",
    "print('accuracy after editing, one answer match is good', \n",
    "      round_percent(len(accuracy_ind_after) / len_edit_val), len(accuracy_ind_after))\n",
    "\n",
    "accuracy_ind_before_collapsed = [i for i in range(len(set(qid_edit_val))) if ans_vocab_list[collapsed_pred_ans_val[i]] in collapsed_all_ans_val[i]]\n",
    "print('accuracy before editing_collapsed', \n",
    "      round_percent(len(accuracy_ind_before_collapsed) / len(set(qid_edit_val))))\n",
    "\n",
    "# accuracy_ind_before_c = [i for i in collapsed_indices if ans_vocab_list[pred_ans_val[i]] in all_ans_val[i]]\n",
    "# print('accuracy before editing, one answer match is good', \n",
    "#       round_percent(len(accuracy_ind_before_c) /len(set(qid_edit_val))))\n",
    "\n",
    "worst_case_acc(qid_predans_edit_val.keys(),qid_predans_idx_edit_val, \\\n",
    "               qid_predans_edit_val,ans_vocab_list, qid_gt_ans_label, if_print=True)\n",
    "#worst_case_acc(qid_predans_val.keys(), qid_predans_val,ans_vocab_list, qid_gt_ans_label)\n",
    "#ch_atleast_once(qid_predans_edit_val.keys(), qid_predans_edit_val, qid_predans_val ,ans_vocab_list)\n",
    "\n",
    "\n",
    "##official_way\n",
    "off_score_val = vqa_score_list(all_ans_val, pred_ans_val, ans_vocab_list)\n",
    "off_score_val_extended = vqa_score_list(all_ans_edit_val, extended_pred_ans_val, ans_vocab_list)\n",
    "off_score_val_collapsed = vqa_score_list(collapsed_all_ans_val, collapsed_pred_ans_val, ans_vocab_list)\n",
    "off_score_edit_val = vqa_score_list(all_ans_edit_val, pred_ans_edit_val, ans_vocab_list)\n",
    "print('official accuracy before editing', round_percent(np.sum(off_score_val) / len_val))\n",
    "print('official accuracy before editing_all_10_ans_same', round_percent(len(accuracy_ind_bf_10_ans_same) / len(ind_where_10_ans_same)), len(ind_where_10_ans_same))\n",
    "\n",
    "\n",
    "\n",
    "print('official accuracy before editing_extended', round_percent(np.sum(off_score_val_extended) / len_edit_val))\n",
    "print('official accuracy after editing', round_percent(np.sum(off_score_edit_val) / len_edit_val))\n",
    "print('official accuracy before editing_collapsed', round_percent(np.sum(off_score_val_collapsed) / len(collapsed_indices)))\n",
    "\n",
    "# ##SNMN- authors way\n",
    "# author_val_acc = np.sum([pred_ans_val[i] == gt_ans_used_val[i] for i in range(len_val)]) / len_val\n",
    "# author_val_extended_acc = np.sum([extended_pred_ans_val[i] == gt_ans_used_edit_val[i] for i in range(len_edit_val)]) / len_edit_val\n",
    "# author_edit_val_acc = np.sum([pred_ans_edit_val[i] == gt_ans_used_edit_val[i] for i in range(len_edit_val)]) / len_edit_val\n",
    "# print('authors accuracy before editing', round_percent(author_val_acc))\n",
    "# print('authors accuracy before editing_extended', round_percent(author_val_extended_acc))\n",
    "# print('authors accuracy after editing', round_percent(author_edit_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#labels flipped for entire set= 7059        6.522 %\n",
      "#labels remained same= 101180         93.478 %\n",
      "#labels flipped that show a positive change 2758           2.548 %\n",
      "#labels flipped that show a negative change 3084           2.849 %\n",
      "#labels flipped that are both right 0           0.0 %\n",
      "#labels flipped that are both wrong 1217           1.124 %\n",
      "#labels same that are both right 67755           62.598 %\n",
      "#labels same that are both wrong 33425           30.881 %\n"
     ]
    }
   ],
   "source": [
    "### for entire set counting how mnay labels flipped\n",
    "labels_flipped_count = np.sum([extended_pred_ans_val[i] != val for i, val in enumerate(pred_ans_edit_val)])\n",
    "labels_remained_same_count = np.sum([extended_pred_ans_val[i] == val for i, val in enumerate(pred_ans_edit_val)])\n",
    "assert (labels_flipped_count + labels_remained_same_count == len(pred_ans_edit_val))\n",
    "print('#labels flipped for entire set=', labels_flipped_count,'      ' ,\n",
    "      round_percent(labels_flipped_count / len_edit_val), '%')\n",
    "      \n",
    "print('#labels remained same=', labels_remained_same_count,'       ', \n",
    "      round_percent(labels_remained_same_count / len_edit_val), '%')\n",
    "\n",
    "lab_fl_ind = [i for i in range(len_edit_val) if extended_pred_ans_val[i] != pred_ans_edit_val[i]]\n",
    "# label i.e ans was wrong before- right now- one match to 10gt ans is okay\n",
    "lab_fl_pos = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] not in all_ans_edit_val[i] and ans_vocab_list[pred_ans_edit_val[i]] in all_ans_edit_val[i]]\n",
    "# label i.e ans was right before- now wrong - one match to 10gt ans is okay\n",
    "lab_fl_neg = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] in all_ans_edit_val[i] and ans_vocab_list[pred_ans_edit_val[i]] not in all_ans_edit_val[i]]\n",
    "lab_fl_right = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] in all_ans_edit_val[i] and ans_vocab_list[pred_ans_edit_val[i]] in all_ans_edit_val[i]]\n",
    "lab_fl_wrong = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] not in all_ans_edit_val[i] and ans_vocab_list[pred_ans_edit_val[i]] not in all_ans_edit_val[i]]\n",
    "print('#labels flipped that show a positive change', len(lab_fl_pos), '         ', \n",
    "      round_percent(len(lab_fl_pos) / len_edit_val), '%' )\n",
    "print('#labels flipped that show a negative change', len(lab_fl_neg), '         ', \n",
    "      round_percent(len(lab_fl_neg) / len_edit_val), '%')\n",
    "print('#labels flipped that are both right', len(lab_fl_right), '         ', round_percent(len(lab_fl_right) / len_edit_val), '%')\n",
    "print('#labels flipped that are both wrong', len(lab_fl_wrong), '         ', round_percent(len(lab_fl_wrong) / len_edit_val), '%')\n",
    "assert(len(lab_fl_pos) + len(lab_fl_neg) + len(lab_fl_right) + len(lab_fl_wrong) == len(lab_fl_ind))\n",
    "\n",
    "\n",
    "lab_sm_ind = [i for i in range(len_edit_val) if extended_pred_ans_val[i] == pred_ans_edit_val[i]]\n",
    "# labels that remained exactly same- so two cases possible- either right/wrong\n",
    "lab_sm_right = [i for i in lab_sm_ind if ans_vocab_list[pred_ans_edit_val[i]] in all_ans_edit_val[i]]\n",
    "lab_sm_wrong = [i for i in lab_sm_ind if ans_vocab_list[pred_ans_edit_val[i]] not in all_ans_edit_val[i]]\n",
    "print('#labels same that are both right', len(lab_sm_right), '         ', round_percent(len(lab_sm_right)/len_edit_val), '%')\n",
    "print('#labels same that are both wrong', len(lab_sm_wrong), '         ', round_percent(len(lab_sm_wrong) / len_edit_val), '%')\n",
    "assert (len(lab_sm_right) + len(lab_sm_wrong) == len(lab_sm_ind))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}