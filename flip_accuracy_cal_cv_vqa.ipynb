{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import config\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "## set seeds\n",
    "from utils_picking import round_percent\n",
    "import random\n",
    "random.seed(1234)\n",
    "import numpy as np; np.random.seed(1234)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MAKE CHANGES HERE\n",
    "MODEL = 'SNMN'   # alternatively can put \"SAAA\", \"CNN_LSTM\" "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEST_SPLIT = 'val2014'\n",
    "orig_root_dir_qa = config.cv_qa_test_dir_orig\n",
    "edit_root_dir_qa = config.cv_qa_test_dir_edit\n",
    "\n",
    "#### questions keys: 'image_id', 'question', 'question_id'\n",
    "res_file_q = 'v2_OpenEnded_mscoco_' + TEST_SPLIT + '_questions.json'\n",
    "standard_questions_val_json = os.path.join(orig_root_dir_qa, res_file_q)\n",
    "standard_questions_edit_val_json = os.path.join(edit_root_dir_qa, res_file_q)\n",
    "\n",
    "## ann keys: 'image_id', 'question_id', 'answers' , 'multiple_choice_answer'(the most frequent answer), 'question_type', 'answer_type'\n",
    "res_file_a = 'v2_mscoco_' + TEST_SPLIT + '_annotations.json'\n",
    "standard_annotations_val_json = os.path.join(orig_root_dir_qa, res_file_a)\n",
    "standard_annotations_edit_val_json = os.path.join(edit_root_dir_qa, res_file_a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if MODEL == 'SNMN':\n",
    "    results_edit_val= config.results_edit_val_snmn_cv_vqa\n",
    "    results_val= config.results_val_snmn\n",
    "    ## standard_vocab file_model_specific\n",
    "    standard_vocab_ans_file = config.standard_vocab_ans_file_snmn\n",
    "    with open(standard_vocab_ans_file) as f:\n",
    "        ans_vocab_list = f.read().splitlines()\n",
    "    \n",
    "elif MODEL == 'SAAA':\n",
    "    results_edit_val = config.results_edit_val_saaa_cv_vqa\n",
    "    results_val = config.results_val_saaa\n",
    "    ## standard_vocab file_model_specific\n",
    "    vocab = config.standard_vocab_ans_file_saaa\n",
    "    with open(vocab, 'r') as f:\n",
    "        ans_vocab = json.load(f)[\"answer\"]\n",
    "        ans_vocab_list = [k for k, v in ans_vocab.items()]\n",
    "        #{v: k for k, v in ans_vocab.items()}   ### is a dictionary here but will work: keys- index- 0,1,2...\n",
    "\n",
    "elif MODEL == 'CNN_LSTM':\n",
    "    results_edit_val = config.results_edit_val_cnn_lstm_cv_vqa\n",
    "    results_val = config.results_val_cnn_lstm\n",
    "    ## standard_vocab file_model_specific\n",
    "    vocab = config.standard_vocab_ans_file_cnn_lstm\n",
    "    with open(vocab, 'r') as f:\n",
    "        ans_vocab = json.load(f)[\"answer\"]\n",
    "        ans_vocab_list = [k for k, v in ans_vocab.items()]\n",
    "        # {v: k for k, v in ans_vocab.items()}   ### is a dictionary here but will work: keys- index- 0,1,2...\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_read_old(results_old_pkl, standard_q_json, standard_a_json=None):\n",
    "    st = time.time()\n",
    "    with open(results_old_pkl, 'rb') as file:\n",
    "        res_val = pickle.load(file)\n",
    "    if str(standard_q_json)[-4:] == 'json':\n",
    "        with open(standard_q_json) as file:\n",
    "            details_ques = json.load(file)['questions']\n",
    "    else:\n",
    "        with open(standard_q_json, 'rb') as file:\n",
    "            details_ques = pickle.load(file)['questions']\n",
    "\n",
    "    ## from results file- model specific\n",
    "    pred_ans = [details['ans_id'] for details in res_val]\n",
    "    softmax_vector = [details['ss_vc'] for details in res_val]\n",
    "    img_id_res = [details['img_id'] for details in res_val]\n",
    "    qid_res = [details['ques_id'] for details in res_val]\n",
    "    # gt_ans_used = [details['gt_ans_id_used'] for details in res_val] ## in case of snmn\n",
    "    # q_ids = [details['ques_id'] for details in res_val]     ### in cases of snmn\n",
    "    # assert (q_ids == [details_q['question_id'] for details_q in details_ques])\n",
    "\n",
    "    q_ids = [details_q['question_id'] for details_q in details_ques]\n",
    "    img_ids = [details_q['image_id'] for details_q in details_ques]\n",
    "    img_ids = [str(iid).zfill(12) for iid in img_ids]\n",
    "    ques_str = [details_q['question'] for details_q in details_ques]\n",
    "\n",
    "    ### make sure order of qid, img_id consistent between results and standard annotations files\n",
    "    ## if not, sort them to what is there in the standard files\n",
    "    #ipdb.set_trace()\n",
    "    if qid_res != q_ids and img_id_res != img_ids:\n",
    "        std_q_img_id = {}\n",
    "\n",
    "        ### SAA and CNNN_LSTM in case for train set: we find a discrpancy so qids are nto a strict subset of qid_res\n",
    "        if len((set(q_ids) & set(qid_res))) != len(set(q_ids)):\n",
    "            print()\n",
    "            print(' passing wrong file- please check the std_ques and results files: DISCREPANCY ALERT')\n",
    "            print()\n",
    "            target_q_ids = list((set(q_ids) & set(qid_res)))\n",
    "        else:    ## ideally q_ids is a strict subset of res_qids...\n",
    "            target_q_ids = q_ids\n",
    "\n",
    "        for qid_idx, qid in enumerate(target_q_ids):\n",
    "            key = str(qid) +  str(img_ids[qid_idx])\n",
    "            std_q_img_id[key] = qid_idx\n",
    "\n",
    "        res_q_img_id_pans_ss_vc = {}\n",
    "        for qid_idx, qid in enumerate(qid_res):\n",
    "            # if 'torch' in img_id_res[qid_idx].type():\n",
    "            #     img_id_res[qid_idx] = img_id_res[qid_idx].item()\n",
    "            # if len(str(img_id_res[qid_idx])) < 20:  ## SAAA fix; for edit_set- img_id is string and not torch object\n",
    "            #     img_id_res[qid_idx] = img_id_res[qid_idx].item()\n",
    "            key_new = str(qid) + str(img_id_res[qid_idx])\n",
    "            res_q_img_id_pans_ss_vc[key_new] = pred_ans[qid_idx], softmax_vector[qid_idx], qid, img_id_res[qid_idx]\n",
    "\n",
    "        pred_ans_corr = [res_q_img_id_pans_ss_vc[key][0] for key in std_q_img_id.keys()]\n",
    "        softmax_vector_corr = [res_q_img_id_pans_ss_vc[key][1] for key in std_q_img_id.keys()]\n",
    "        qid_corr = [res_q_img_id_pans_ss_vc[key][2] for key in std_q_img_id.keys()]\n",
    "        img_id_corr = [res_q_img_id_pans_ss_vc[key][3] for key in std_q_img_id.keys()]\n",
    "        assert img_id_corr == img_ids\n",
    "        assert qid_corr == q_ids\n",
    "\n",
    "        pred_ans = pred_ans_corr\n",
    "        softmax_vector = softmax_vector_corr\n",
    "\n",
    "\n",
    "    if standard_a_json is not None:\n",
    "        with open(standard_a_json) as file:\n",
    "            details_ann = json.load(file)['annotations']\n",
    "        all_answers = [[ans['answer'] for ans in details_ann[i]['answers']] for i in range(len(img_ids))]\n",
    "        # most_freq_ans = [details_ann[i]['multiple_choice_answer'] for i in range(len(img_ids))]\n",
    "        ques_type_data = [details_ann[i]['question_type'] for i in range(len(img_ids))]\n",
    "        ans_type_data = [details_ann[i]['answer_type'] for i in range(len(img_ids))]\n",
    "        print(time.time() - st)\n",
    "        return q_ids, pred_ans, softmax_vector, img_ids, ques_str, all_answers, ques_type_data, ans_type_data     # ,gt_ans_used\n",
    "    else:\n",
    "        print(time.time() - st)\n",
    "        return q_ids, pred_ans, softmax_vector, img_ids, ques_str, [], [], []  # ,gt_ans_used"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qid_val, pred_ans_val , ss_vc_val, img_ids_val, ques_val, all_ans_val, ques_type_val, ans_type_val = my_read_old(results_val, standard_questions_val_json, standard_annotations_val_json)\n",
    "\n",
    "qid_edit_val, pred_ans_edit_val , ss_vc_edit_val, img_ids_edit_val, ques_edit_val, all_ans_edit_val, ques_type_edit_val, ans_type_edit_val = my_read_old(results_edit_val, standard_questions_edit_val_json, standard_annotations_edit_val_json)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### since we are taking just val_90_10: for testing purpose here: so there are some del_1 ques_ids who dont have\n",
    "## corresponding original IQAs because we just took the first 90 split of val.\n",
    "\n",
    "## BETTER TO RESTRICT THE QID_EDIT_VAL!!!\n",
    "\n",
    "### HARD-CODED!\n",
    "# print('REMINDER: only for val_90_10, just taking first 3743 indices of del_1, this is hard-coded')\n",
    "# assert [qid_edit_val[i] not in qid_ss_predans_val.keys() for i in range(3743,len(qid_edit_val),1)]\n",
    "# qid_edit_val = qid_edit_val[0:3743]\n",
    "\n",
    "# chunk_these_lists = [qid_edit_val, pred_ans_edit_val , ss_vc_edit_val, img_ids_edit_val, ques_edit_val, all_ans_edit_val, ques_type_edit_val, ans_type_edit_val]\n",
    "# chunked_lists = [i[0:3743] for i in chunk_these_lists]\n",
    "# qid_edit_val, pred_ans_edit_val , ss_vc_edit_val, img_ids_edit_val, ques_edit_val, all_ans_edit_val, ques_type_edit_val, ans_type_edit_val = chunked_lists\n",
    "\n",
    "\n",
    "masking_indices_where_qid_edit_but_no_orig_qid = [idx for idx, i in enumerate(qid_edit_val) if i not in qid_val]\n",
    "stop_idx = masking_indices_where_qid_edit_but_no_orig_qid[0]\n",
    "qid_edit_val = qid_edit_val[0:stop_idx]\n",
    "qid_edit_val, pred_ans_edit_val , ss_vc_edit_val, img_ids_edit_val, ques_edit_val, all_ans_edit_val, ques_type_edit_val, ans_type_edit_val = [i[0:stop_idx] for i in [qid_edit_val, pred_ans_edit_val , ss_vc_edit_val, img_ids_edit_val, ques_edit_val, all_ans_edit_val, ques_type_edit_val, ans_type_edit_val]]\n",
    "\n",
    "\n",
    "len_val = len(img_ids_val)\n",
    "len_edit_val = len(img_ids_edit_val)\n",
    "all_indices_val = np.arange(len_val)\n",
    "all_indices_edit_val = np.arange(len_edit_val)\n",
    "\n",
    "print('size of original set:', len_val)\n",
    "print('size of edited set:', len_edit_val)\n",
    "print('no of unique images in val_set: ',len(set(img_ids_val)))\n",
    "print('no of unique images in edit_val_set: ',len(set(img_ids_edit_val)))\n",
    "print('#unique questions in original set: ',len(list(set(qid_val))) )\n",
    "print('#unique questions in edited set: ', len(list(set(qid_edit_val))))\n",
    "\n",
    "if len(list(set(qid_val))) != len(list(set(qid_edit_val))) :\n",
    "    print('not every question in orig_val made it to edit_val=> for ', len(list(set(qid_val)))- len(list(set(qid_edit_val)))  ,' questions- no legit edited IQA possible')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qid_gt_ans_label = {}\n",
    "for idx,a in enumerate(qid_val):\n",
    "    qid_gt_ans_label[a] = all_ans_val[idx]\n",
    "    \n",
    "qid_predans_val = {}\n",
    "for idx, a in enumerate(qid_val):\n",
    "    qid_predans_val.setdefault(a, []).append(pred_ans_val[idx])\n",
    "    \n",
    "qid_predans_idx_val = {}\n",
    "for idx, a in enumerate(qid_val):\n",
    "    qid_predans_idx_val.setdefault(a, []).append(idx)\n",
    "    \n",
    "#idx in case here refers to len(val and edit_val- order hai - so relax)    \n",
    "qid_predans_edit_val = {}\n",
    "for idx, a in enumerate(qid_edit_val):\n",
    "    qid_predans_edit_val.setdefault(a, []).append(pred_ans_edit_val[idx])\n",
    "    #qid_predans_edit_val[a] = (pred_ans_edit_val[idx])   \n",
    "    \n",
    "qid_predans_idx_edit_val = {}\n",
    "for idx, a in enumerate(qid_edit_val):\n",
    "    qid_predans_idx_edit_val.setdefault(a, []).append(idx)    \n",
    "    \n",
    "# qid_predans_imgid_edit_val = {}\n",
    "# for idx, a in enumerate(qid_edit_val):\n",
    "#     qid_predans_imgid_edit_val.setdefault(a, []).append(img_ids_edit_val[idx])   \n",
    "\n",
    "## creating dictionary for val set - to facilitate extensions based on q_id index\n",
    "qid_ss_predans_val = {}\n",
    "for idx, a in enumerate(qid_val):\n",
    "    qid_ss_predans_val[a] = (ss_vc_val[idx], pred_ans_val[idx], qid_val[idx],idx, all_ans_val[idx] )\n",
    "\n",
    "extended_ss_vc_val = [qid_ss_predans_val[q_id][0] for q_id in qid_edit_val ]\n",
    "extended_pred_ans_val = [qid_ss_predans_val[q_id][1] for q_id in qid_edit_val ] \n",
    "extended_gt_ans_val = [qid_gt_ans_label[q_id][0] for q_id in qid_edit_val ] \n",
    "extended_qid_val = [qid_ss_predans_val[q_id][2] for q_id in qid_edit_val]\n",
    "\n",
    "collapsed_pred_ans_val= [qid_ss_predans_val[q_id][1] for q_id in set(qid_edit_val) ] \n",
    "\n",
    "collapsed_gt_ans_val = [qid_ss_predans_val[q_id][4][0] for q_id in set(qid_edit_val)] \n",
    "collapsed_indices = [qid_ss_predans_val[q_id][3] for q_id in set(qid_edit_val)] \n",
    "\n",
    "gt_ans_edit_val = [i[0] for i in all_ans_edit_val]\n",
    "gt_ans_val = [i[0] for i in all_ans_val]\n",
    "### all_10_ans same\n",
    "## extended_gt_ans_edit_val\n",
    "## extended_gt_ans_val\n",
    "\n",
    "\n",
    "assert [int(gt_ans_edit_val[i])+1== int(extended_gt_ans_val[i]) for i in range(len(gt_ans_edit_val))]\n",
    "\n",
    "assert len(collapsed_indices) == len(set(qid_edit_val))\n",
    "\n",
    "\n",
    "##### pred: predictions: need to  be mapped to ans_vocab\n",
    "####pred_ans/gt_ans: has to be string!!!!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###                                         STATISTICS\n",
    "\n",
    "accuracy_ind_before = [i for i in range(len_val) if ans_vocab_list[pred_ans_val[i]] == gt_ans_val[i]]\n",
    "accuracy_ind_before_extended = [i for i in range(len_edit_val) if ans_vocab_list[extended_pred_ans_val[i]]== extended_gt_ans_val[i]]\n",
    "accuracy_ind_after = [i for i in range(len_edit_val) if ans_vocab_list[pred_ans_edit_val[i]] == gt_ans_edit_val[i]]\n",
    "accuracy_ind_before_collapsed = [i for i in range(len(set(qid_edit_val))) if ans_vocab_list[collapsed_pred_ans_val[i]] == collapsed_gt_ans_val[i]]\n",
    "print('accuracy before editing, one answer match is good', \n",
    "      round_percent(len(accuracy_ind_before) / len_val), len(accuracy_ind_before),'/', len_val)\n",
    "print('accuracy before editing_extended', \n",
    "      round_percent(len(accuracy_ind_before_extended) / len_edit_val), len(accuracy_ind_before_extended), '/', len_edit_val)\n",
    "print('accuracy after editing, one answer match is good', \n",
    "      round_percent(len(accuracy_ind_after) / len_edit_val), len(accuracy_ind_after),'/', len_edit_val)\n",
    "\n",
    "print('accuracy before editing_collapsed', \n",
    "      round_percent(len(accuracy_ind_before_collapsed) / len(set(qid_edit_val))), len(accuracy_ind_before_collapsed),'/', len(set(qid_edit_val)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chuck_many_list = ['many', 'black']\n",
    "chuck_many = [i for i, val in enumerate(pred_ans_edit_val) if ans_vocab_list[val] in chuck_many_list]\n",
    "\n",
    "print('many there in predicted answers, so we would remove them; #many: ', len(chuck_many))\n",
    "\n",
    "if len(chuck_many) != 0:\n",
    "    ### for entire set counting how mnay labels flipped\n",
    "    labels_flipped_count = np.sum([ans_vocab_list[extended_pred_ans_val[i]] != str(int(ans_vocab_list[val]) + 1)\n",
    "                                   if ans_vocab_list[val] not in chuck_many_list else 0 for i, val in\n",
    "                                   enumerate(pred_ans_edit_val)])\n",
    "    # labels_flipped_count = np.sum([ans_vocab_list[extended_pred_ans_val[i]] != str(int(ans_vocab_list[val])+1) for i, val in enumerate(pred_ans_edit_val)])\n",
    "    labels_remained_same_count = np.sum(\n",
    "        [ans_vocab_list[extended_pred_ans_val[i]] == str(int(ans_vocab_list[val]) + 1)\n",
    "         if ans_vocab_list[val] not in chuck_many_list else 0 for i, val in enumerate(pred_ans_edit_val)])\n",
    "    assert (labels_flipped_count + labels_remained_same_count + len(chuck_many) == len(pred_ans_edit_val))\n",
    "\n",
    "    indices_not_many = [i for i, val in enumerate(pred_ans_edit_val) if ans_vocab_list[val] not in chuck_many_list]\n",
    "\n",
    "    lab_fl_ind = [i for i in indices_not_many if ans_vocab_list[extended_pred_ans_val[i]]\n",
    "                  != str(int(ans_vocab_list[pred_ans_edit_val[i]]) + 1)]\n",
    "    lab_fl_pos = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] != extended_gt_ans_val[i]\n",
    "                  and ans_vocab_list[pred_ans_edit_val[i]] == gt_ans_edit_val[i]]\n",
    "    lab_fl_neg = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] == extended_gt_ans_val[i]\n",
    "                  and ans_vocab_list[pred_ans_edit_val[i]] != gt_ans_edit_val[i]]\n",
    "    lab_fl_right = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] == extended_gt_ans_val[i]\n",
    "                    and ans_vocab_list[pred_ans_edit_val[i]] == gt_ans_edit_val[i]]\n",
    "    lab_fl_wrong = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] != extended_gt_ans_val[i]\n",
    "                    and ans_vocab_list[pred_ans_edit_val[i]] != gt_ans_edit_val[i]]\n",
    "    assert (len(lab_fl_pos) + len(lab_fl_neg) + len(lab_fl_right) + len(lab_fl_wrong) == len(lab_fl_ind))\n",
    "\n",
    "    lab_sm_ind = [i for i in indices_not_many if ans_vocab_list[extended_pred_ans_val[i]]\n",
    "                  == str(int(ans_vocab_list[pred_ans_edit_val[i]]) + 1)]\n",
    "    lab_sm_right = [i for i in lab_sm_ind if ans_vocab_list[extended_pred_ans_val[i]] == extended_gt_ans_val[i]\n",
    "                    and ans_vocab_list[pred_ans_edit_val[i]] == gt_ans_edit_val[i]]\n",
    "    lab_sm_wrong = [i for i in lab_sm_ind if ans_vocab_list[extended_pred_ans_val[i]] != extended_gt_ans_val[i]\n",
    "                    and ans_vocab_list[pred_ans_edit_val[i]] != gt_ans_edit_val[i]]\n",
    "    assert (len(lab_sm_right) + len(lab_sm_wrong) == len(lab_sm_ind))\n",
    "\n",
    "\n",
    "else:\n",
    "    ### for entire set counting how mnay labels flipped\n",
    "    labels_flipped_count = np.sum(\n",
    "        [ans_vocab_list[extended_pred_ans_val[i]] != str(int(ans_vocab_list[val]) + 1) for i, val in\n",
    "         enumerate(pred_ans_edit_val)])\n",
    "    # labels_flipped_count = np.sum([ans_vocab_list[extended_pred_ans_val[i]] != str(int(ans_vocab_list[val])+1) for i, val in enumerate(pred_ans_edit_val)])\n",
    "    labels_remained_same_count = np.sum(\n",
    "        [ans_vocab_list[extended_pred_ans_val[i]] == str(int(ans_vocab_list[val]) + 1) for i, val in\n",
    "         enumerate(pred_ans_edit_val)])\n",
    "    assert (labels_flipped_count + labels_remained_same_count == len(pred_ans_edit_val))\n",
    "\n",
    "    lab_fl_ind = [i for i in range(len_edit_val) if ans_vocab_list[extended_pred_ans_val[i]]\n",
    "                  != str(int(ans_vocab_list[pred_ans_edit_val[i]]) + 1)]\n",
    "\n",
    "    lab_fl_pos = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] != extended_gt_ans_val[i]\n",
    "                  and ans_vocab_list[pred_ans_edit_val[i]] == gt_ans_edit_val[i]]\n",
    "    lab_fl_neg = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] == extended_gt_ans_val[i]\n",
    "                  and ans_vocab_list[pred_ans_edit_val[i]] != gt_ans_edit_val[i]]\n",
    "    lab_fl_right = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] == extended_gt_ans_val[i]\n",
    "                    and ans_vocab_list[pred_ans_edit_val[i]] == gt_ans_edit_val[i]]\n",
    "    lab_fl_wrong = [i for i in lab_fl_ind if ans_vocab_list[extended_pred_ans_val[i]] != extended_gt_ans_val[i]\n",
    "                    and ans_vocab_list[pred_ans_edit_val[i]] != gt_ans_edit_val[i]]\n",
    "    assert (len(lab_fl_pos) + len(lab_fl_neg) + len(lab_fl_right) + len(lab_fl_wrong) == len(lab_fl_ind))\n",
    "\n",
    "    lab_sm_ind = [i for i in range(len_edit_val) if ans_vocab_list[extended_pred_ans_val[i]]\n",
    "                  == str(int(ans_vocab_list[pred_ans_edit_val[i]]) + 1)]\n",
    "    lab_sm_right = [i for i in lab_sm_ind if ans_vocab_list[extended_pred_ans_val[i]] == extended_gt_ans_val[i]\n",
    "                    and ans_vocab_list[pred_ans_edit_val[i]] == gt_ans_edit_val[i]]\n",
    "    lab_sm_wrong = [i for i in lab_sm_ind if ans_vocab_list[extended_pred_ans_val[i]] != extended_gt_ans_val[i]\n",
    "                    and ans_vocab_list[pred_ans_edit_val[i]] != gt_ans_edit_val[i]]\n",
    "    assert (len(lab_sm_right) + len(lab_sm_wrong) == len(lab_sm_ind))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## all final answers in string. Dont forget to match the ans+predicted to the vocab used!\n",
    "### for entire set counting how mnay labels flipped\n",
    "\n",
    "print('#labels not n/n-1 conssitent for entire set=', labels_flipped_count,'      ' ,\n",
    "      round_percent(labels_flipped_count / len_edit_val), '%')\n",
    "\n",
    "print('#labels n/n-1 conssitent =', labels_remained_same_count,'       ', \n",
    "      round_percent(labels_remained_same_count / len_edit_val), '%')\n",
    "\n",
    "\n",
    "print('neg->pos', len(lab_fl_pos), '         ', \n",
    "      round_percent(len(lab_fl_pos) / len_edit_val), '%' )\n",
    "print('pos->neg', len(lab_fl_neg), '         ', \n",
    "      round_percent(len(lab_fl_neg) / len_edit_val), '%')\n",
    "print('pos->pos', len(lab_fl_right), '         ', round_percent(len(lab_fl_right) / len_edit_val), '%')\n",
    "print('neg->neg', len(lab_fl_wrong), '         ', round_percent(len(lab_fl_wrong) / len_edit_val), '%')\n",
    "\n",
    "\n",
    "\n",
    "# labels that remained exactly same- so two cases possible- either right/wrong\n",
    "\n",
    "print('#labels consistent both right', len(lab_sm_right), '         ', round_percent(len(lab_sm_right)/len_edit_val), '%')\n",
    "print('#labels consistent both wrong', len(lab_sm_wrong), '         ', round_percent(len(lab_sm_wrong) / len_edit_val), '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}